{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8f3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# import helper\n",
    "import numpy as np\n",
    "# import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27532a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3395691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4870404041812672980\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23089ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r') as fd:\n",
    "      return fd.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b420e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('processed_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f87021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643a2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences=df1['Original']\n",
    "french_sentences=df1['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6a7bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 Delete all elements at application close\n",
       "1        Which one of thesecode samples has better perf...\n",
       "2        Why library name gets an additional UNK NUMBER...\n",
       "3                What have you use Regular Expressions for\n",
       "4        Calculation of cubic bezier with known halfway...\n",
       "                               ...                        \n",
       "22610    How to refresh a webpage automaticly every spe...\n",
       "22611    Help me to complete my UITableView to Navigati...\n",
       "22612    JQuery focus() on jQuery-applied anchors works...\n",
       "22613                 (tcl/Expect) clear screen after exit\n",
       "22614    How to call C# method taking an object of type...\n",
       "Name: Original, Length: 22615, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "470eca36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210794 English words.\n",
      "26489 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"to\" \"a\" \"in\" \"How\" \"the\" \"of\" \"and\" \"for\" \"NUMBER\" \"with\"\n",
      "\n",
      "216743 French words.\n",
      "21989 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"to\" \"a\" \"in\" \"How\" \"the\" \"of\" \"I\" \"and\" \"NUMBER\" \"for\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2275db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    tokenizer = Tokenizer(num_words=None, char_level=True,lower=False)\n",
    "#     tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    print(tokenizer.texts_to_sequences(x))\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15e54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_column_to_list=df1['Original'].tolist()+df1['Target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f065b257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_x, x_tk = tokenize(all_column_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf21661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ' ',\n",
       " 2: 'e',\n",
       " 3: 't',\n",
       " 4: 'o',\n",
       " 5: 'a',\n",
       " 6: 'i',\n",
       " 7: 'n',\n",
       " 8: 'r',\n",
       " 9: 's',\n",
       " 10: 'l',\n",
       " 11: 'c',\n",
       " 12: 'd',\n",
       " 13: 'h',\n",
       " 14: 'u',\n",
       " 15: 'p',\n",
       " 16: 'm',\n",
       " 17: 'g',\n",
       " 18: 'w',\n",
       " 19: 'f',\n",
       " 20: 'y',\n",
       " 21: 'b',\n",
       " 22: 'v',\n",
       " 23: 'S',\n",
       " 24: 'N',\n",
       " 25: 'U',\n",
       " 26: 'I',\n",
       " 27: 'M',\n",
       " 28: 'H',\n",
       " 29: 'C',\n",
       " 30: 'E',\n",
       " 31: 'R',\n",
       " 32: 'k',\n",
       " 33: 'P',\n",
       " 34: 'B',\n",
       " 35: 'x',\n",
       " 36: 'L',\n",
       " 37: 'W',\n",
       " 38: 'A',\n",
       " 39: 'D',\n",
       " 40: 'K',\n",
       " 41: 'T',\n",
       " 42: '-',\n",
       " 43: 'j',\n",
       " 44: 'Q',\n",
       " 45: 'F',\n",
       " 46: 'q',\n",
       " 47: \"'\",\n",
       " 48: 'O',\n",
       " 49: 'J',\n",
       " 50: '(',\n",
       " 51: ':',\n",
       " 52: ',',\n",
       " 53: 'V',\n",
       " 54: ')',\n",
       " 55: '+',\n",
       " 56: '\"',\n",
       " 57: '/',\n",
       " 58: 'G',\n",
       " 59: '#',\n",
       " 60: 'z',\n",
       " 61: 'X',\n",
       " 62: '_',\n",
       " 63: ']',\n",
       " 64: '[',\n",
       " 65: '>',\n",
       " 66: '<',\n",
       " 67: 'Y',\n",
       " 68: '*',\n",
       " 69: '&',\n",
       " 70: 'Z',\n",
       " 71: '$',\n",
       " 72: '`',\n",
       " 73: '@',\n",
       " 74: '%',\n",
       " 75: '\\\\',\n",
       " 76: '|',\n",
       " 77: '~',\n",
       " 78: '^',\n",
       " 79: '\\x7f'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tk.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1446fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_toseq(x,Tok):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "#     tokenizer = Tokenizer()\n",
    "    Tok.fit_on_texts(x)\n",
    "#     print(Tok.texts_to_sequences(x))\n",
    "    return Tok.texts_to_sequences(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35bfdc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "# test_pad(pad)\n",
    "\n",
    "# # Pad Tokenized output\n",
    "# test_pad = pad(text_tokenized)\n",
    "# for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "#     print('Sequence {} in x'.format(sample_i + 1))\n",
    "#     print('  Input:  {}'.format(np.array(token_sent)))\n",
    "#     print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa92df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_english_sentences1, preproc_french_sentences1, english_tokenizer1, french_tokenizer =\\\n",
    "    preprocess1(english_sentences, french_sentences,x_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5173950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess1(x,y,x_tk):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x = tokenize_toseq(x,x_tk)\n",
    "    preprocess_y = tokenize_toseq(y,x_tk)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    return preprocess_x, preprocess_y, x_tk, x_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6869d1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ' ',\n",
       " 2: 'e',\n",
       " 3: 't',\n",
       " 4: 'o',\n",
       " 5: 'a',\n",
       " 6: 'i',\n",
       " 7: 'n',\n",
       " 8: 'r',\n",
       " 9: 's',\n",
       " 10: 'l',\n",
       " 11: 'c',\n",
       " 12: 'd',\n",
       " 13: 'h',\n",
       " 14: 'u',\n",
       " 15: 'p',\n",
       " 16: 'm',\n",
       " 17: 'g',\n",
       " 18: 'w',\n",
       " 19: 'f',\n",
       " 20: 'y',\n",
       " 21: 'b',\n",
       " 22: 'v',\n",
       " 23: 'S',\n",
       " 24: 'N',\n",
       " 25: 'U',\n",
       " 26: 'I',\n",
       " 27: 'M',\n",
       " 28: 'H',\n",
       " 29: 'C',\n",
       " 30: 'E',\n",
       " 31: 'R',\n",
       " 32: 'k',\n",
       " 33: 'P',\n",
       " 34: 'B',\n",
       " 35: 'x',\n",
       " 36: 'L',\n",
       " 37: 'W',\n",
       " 38: 'A',\n",
       " 39: 'D',\n",
       " 40: 'K',\n",
       " 41: 'T',\n",
       " 42: '-',\n",
       " 43: 'j',\n",
       " 44: 'Q',\n",
       " 45: 'F',\n",
       " 46: 'q',\n",
       " 47: \"'\",\n",
       " 48: 'O',\n",
       " 49: 'J',\n",
       " 50: '(',\n",
       " 51: ':',\n",
       " 52: ',',\n",
       " 53: 'V',\n",
       " 54: ')',\n",
       " 55: '+',\n",
       " 56: '\"',\n",
       " 57: '/',\n",
       " 58: 'G',\n",
       " 59: '#',\n",
       " 60: 'z',\n",
       " 61: 'X',\n",
       " 62: '_',\n",
       " 63: ']',\n",
       " 64: '[',\n",
       " 65: '>',\n",
       " 66: '<',\n",
       " 67: 'Y',\n",
       " 68: '*',\n",
       " 69: '&',\n",
       " 70: 'Z',\n",
       " 71: '$',\n",
       " 72: '`',\n",
       " 73: '@',\n",
       " 74: '%',\n",
       " 75: '\\\\',\n",
       " 76: '|',\n",
       " 77: '~',\n",
       " 78: '^',\n",
       " 79: '\\x7f'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokenizer1.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed6d31ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ' ',\n",
       " 2: 'e',\n",
       " 3: 't',\n",
       " 4: 'o',\n",
       " 5: 'a',\n",
       " 6: 'i',\n",
       " 7: 'n',\n",
       " 8: 'r',\n",
       " 9: 's',\n",
       " 10: 'l',\n",
       " 11: 'c',\n",
       " 12: 'd',\n",
       " 13: 'h',\n",
       " 14: 'u',\n",
       " 15: 'p',\n",
       " 16: 'm',\n",
       " 17: 'g',\n",
       " 18: 'w',\n",
       " 19: 'f',\n",
       " 20: 'y',\n",
       " 21: 'b',\n",
       " 22: 'v',\n",
       " 23: 'S',\n",
       " 24: 'N',\n",
       " 25: 'U',\n",
       " 26: 'I',\n",
       " 27: 'M',\n",
       " 28: 'H',\n",
       " 29: 'C',\n",
       " 30: 'E',\n",
       " 31: 'R',\n",
       " 32: 'k',\n",
       " 33: 'P',\n",
       " 34: 'B',\n",
       " 35: 'x',\n",
       " 36: 'L',\n",
       " 37: 'W',\n",
       " 38: 'A',\n",
       " 39: 'D',\n",
       " 40: 'K',\n",
       " 41: 'T',\n",
       " 42: '-',\n",
       " 43: 'j',\n",
       " 44: 'Q',\n",
       " 45: 'F',\n",
       " 46: 'q',\n",
       " 47: \"'\",\n",
       " 48: 'O',\n",
       " 49: 'J',\n",
       " 50: '(',\n",
       " 51: ':',\n",
       " 52: ',',\n",
       " 53: 'V',\n",
       " 54: ')',\n",
       " 55: '+',\n",
       " 56: '\"',\n",
       " 57: '/',\n",
       " 58: 'G',\n",
       " 59: '#',\n",
       " 60: 'z',\n",
       " 61: 'X',\n",
       " 62: '_',\n",
       " 63: ']',\n",
       " 64: '[',\n",
       " 65: '>',\n",
       " 66: '<',\n",
       " 67: 'Y',\n",
       " 68: '*',\n",
       " 69: '&',\n",
       " 70: 'Z',\n",
       " 71: '$',\n",
       " 72: '`',\n",
       " 73: '@',\n",
       " 74: '%',\n",
       " 75: '\\\\',\n",
       " 76: '|',\n",
       " 77: '~',\n",
       " 78: '^',\n",
       " 79: '\\x7f'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "688896b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 183\n",
      "Max French sentence length: 180\n",
      "English vocabulary size: 79\n",
      "French vocabulary size: 79\n"
     ]
    }
   ],
   "source": [
    "max_english_sequence_length = preproc_english_sentences1.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences1.shape[1]\n",
    "english_vocab_size = len(english_tokenizer1.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "852de47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense,  RepeatVector, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43ecaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x1 = tf.keras.utils.to_categorical(preproc_english_sentences1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a89e400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = tf.keras.utils.to_categorical(preproc_english_sentences1)\n",
    "tmp_y = tf.keras.utils.to_categorical(preproc_french_sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9518d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, original_vocab_size, target_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param original_vocab_size: Number of unique character in original text\n",
    "    :param target_vocab_size: Number of unique character in target text\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=input_shape))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(SimpleRNN(1024, return_sequences=True))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(target_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c434d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0833142",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Train and Print prediction(s)\n",
    "encdec_rnn_model = encdec_model(\n",
    "    tmp_x.shape[1:],\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size + 1,\n",
    "    french_vocab_size )\n",
    "\n",
    "encdec_rnn_model.summary()\n",
    "\n",
    "encdec_rnn_model.fit(tmp_x, tmp_y, batch_size=32, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c24ebb",
   "metadata": {},
   "source": [
    "encdec_rnn_model.save('Model_char_lvl_Org_Mode.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d49ee8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "encdec_rnn_model = load_model('Model_char_lvl_Org_Mode.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "304bee38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tokenizer.to_json of <keras_preprocessing.text.Tokenizer object at 0x000001AFD97417F0>>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tk.to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a098d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0658ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9db0bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = x_tk.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5ae420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_from_json(json_string):\n",
    "  \"\"\"Parses a JSON tokenizer configuration and returns a tokenizer instance.\n",
    "  Deprecated: `tf.keras.preprocessing.text.Tokenizer` does not operate on\n",
    "  tensors and is not recommended for new code. Prefer\n",
    "  `tf.keras.layers.TextVectorization` which provides equivalent functionality\n",
    "  through a layer which accepts `tf.Tensor` input. See the\n",
    "  [text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text)\n",
    "  for an overview of the layer and text handling in tensorflow.\n",
    "  Args:\n",
    "      json_string: JSON string encoding a tokenizer configuration.\n",
    "  Returns:\n",
    "      A Keras Tokenizer instance\n",
    "  \"\"\"\n",
    "  tokenizer_config = json.loads(json_string)\n",
    "  config = tokenizer_config.get('config')\n",
    "\n",
    "  word_counts = json.loads(config.pop('word_counts'))\n",
    "  word_docs = json.loads(config.pop('word_docs'))\n",
    "  index_docs = json.loads(config.pop('index_docs'))\n",
    "  # Integer indexing gets converted to strings with json.dumps()\n",
    "  index_docs = {int(k): v for k, v in index_docs.items()}\n",
    "  index_word = json.loads(config.pop('index_word'))\n",
    "  index_word = {int(k): v for k, v in index_word.items()}\n",
    "  word_index = json.loads(config.pop('word_index'))\n",
    "\n",
    "  tokenizer = Tokenizer(**config)\n",
    "  tokenizer.word_counts = word_counts\n",
    "  tokenizer.word_docs = word_docs\n",
    "  tokenizer.index_docs = index_docs\n",
    "  tokenizer.word_index = word_index\n",
    "  tokenizer.index_word = index_word\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6570d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3501fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# import helper\n",
    "import numpy as np\n",
    "# import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense,  RepeatVector, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c18845a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01fd5e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocess_x1 \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_toseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_tk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mtokenize_toseq\u001b[1;34m(x, Tok)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Tokenize x\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    :param x: List of sentences/strings to be tokenized\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# TODO: Implement\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#     tokenizer = Tokenizer()\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mTok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_on_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     print(Tok.texts_to_sequences(x))\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tok\u001b[38;5;241m.\u001b[39mtexts_to_sequences(x)\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\keras_preprocessing\\text.py:212\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_on_texts\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;124;03m\"\"\"Updates internal vocabulary based on a list of texts.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m    In the case where texts contains lists,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m            or a list of list of strings.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_level \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not iterable"
     ]
    }
   ],
   "source": [
    "preprocess_x1 = tokenize_toseq(io,x_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e200bd75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpreprocess_x1\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_x1' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f78a79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_x1 = pad(preprocess_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "512c9460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37],\n",
       "       [13],\n",
       "       [ 6],\n",
       "       [11],\n",
       "       [13],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [19],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [13],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 2],\n",
       "       [11],\n",
       "       [ 4],\n",
       "       [12],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [16],\n",
       "       [15],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [13],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [21],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 1],\n",
       "       [15],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [19],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [16],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [11],\n",
       "       [ 2]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2324f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x1 = tf.keras.utils.to_categorical(preprocess_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b273ad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "018b3b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////////\n",
      "[[37 13  6 11 13  1  4  7  2  1  4 19  1  3 13  2  9  2 11  4 12  2  1  9\n",
      "   5 16 15 10  2  9  1 13  5  9  1 21  2  3  3  2  8  1 15  2  8 19  4  8\n",
      "  16  5  7 11  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "///////\n",
      "???\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 366 into shape (1,2,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(RX\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentence\n\u001b[1;32m---> 37\u001b[0m xi\u001b[38;5;241m=\u001b[39m\u001b[43msentence_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36msentence_pred\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     16\u001b[0m sentences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sentence[\u001b[38;5;241m0\u001b[39m], preproc_english_sentences1[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m???\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msentences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m???\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m predictions \u001b[38;5;241m=\u001b[39m encdec_rnn_model\u001b[38;5;241m.\u001b[39mpredict(sentences, \u001b[38;5;28mlen\u001b[39m(sentences))\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 366 into shape (1,2,3)"
     ]
    }
   ],
   "source": [
    "def sentence_pred(data):\n",
    "    \n",
    "#     print(data)\n",
    "    sentence=[tokenizer.word_index[ch] for ch in data]\n",
    "    oo=sentence\n",
    "#     print(\"???\")\n",
    "# #     print(oo)\n",
    "#     print(\"???\")\n",
    "#     sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=preproc_english_sentences1.shape[-1], padding='post')\n",
    "    print(\"////////\")\n",
    "\n",
    "    print(sentence)\n",
    "    \n",
    "    print(\"///////\")\n",
    "    sentences = np.array([sentence[0], preproc_english_sentences1[0]])\n",
    "    print(\"???\")\n",
    "\n",
    "    print(sentences.reshape(1,2,3))\n",
    "   \n",
    "    print(\"???\")\n",
    "    predictions = encdec_rnn_model.predict(sentences, len(sentences))\n",
    "    RX=\"\"\n",
    "    for prediction in np.argmax(predictions[0], 1):\n",
    "        print(prediction)\n",
    "        dc=logits_to_text(prediction,x_tk)\n",
    "        dc=str(dc)\n",
    "#         print(dc)\n",
    "        if dc!=None:\n",
    "#             RX+=dc+\" \"\n",
    "            RX+=dc\n",
    "#     print(RX) \n",
    "    sentence = \" \".join(RX.split())\n",
    "    return sentence\n",
    "\n",
    "\n",
    "xi=sentence_pred(io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88193160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits,tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "#     print(tokenizer.index_word.get(logits))\n",
    "    \n",
    "    r=tokenizer.index_word.get(logits)\n",
    "\n",
    "    if r!=None:\n",
    "        r=str(r)\n",
    "    else:\n",
    "        r=\"\"\n",
    "    return r\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ed0ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "io=df['Original'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e79b6582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which one of thesecode samples has better performance'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bf7af44",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1537 predict_step\n        return self(x, training=False)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\input_spec.py:214 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (2, 183)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xi\u001b[38;5;241m=\u001b[39m\u001b[43msentence_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36msentence_pred\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     16\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sentence[\u001b[38;5;241m0\u001b[39m], preproc_english_sentences1[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     print(\"???\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     print(sentences)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#     print(\"???\")\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mencdec_rnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     RX\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1751\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1750\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 1751\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1753\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    936\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:759\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 759\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_internal_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    763\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3066\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 3066\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3463\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3460\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3463\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3298\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3293\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3294\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3295\u001b[0m ]\n\u001b[0;32m   3296\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3297\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3301\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3303\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3306\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3307\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3310\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3311\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3312\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3315\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1007\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1007\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1012\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:668\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    665\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    666\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 668\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    669\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:994\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    993\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    995\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:1537 predict_step\n        return self(x, training=False)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\subra\\Anaconda\\lib\\site-packages\\keras\\engine\\input_spec.py:214 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (2, 183)\n"
     ]
    }
   ],
   "source": [
    "xi=sentence_pred(io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4526f9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8a830b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61c246f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[8.68646530e-05, 8.45148170e-04, 2.48419214e-03, ...,\n",
       "         1.13727707e-04, 8.14071609e-05, 1.02118742e-04],\n",
       "        [1.69759107e-04, 6.60383003e-03, 9.06272680e-02, ...,\n",
       "         6.68481562e-06, 8.84967722e-06, 6.52177778e-06],\n",
       "        [3.92694463e-04, 7.81763494e-02, 6.41846359e-02, ...,\n",
       "         1.39410731e-05, 1.04691599e-05, 6.84323822e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57941983e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75462866e-04, 1.69926381e-04, ...,\n",
       "         1.57768042e-07, 4.25026379e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607730e-07, 4.24554401e-08, 5.40797309e-08]],\n",
       "\n",
       "       [[8.68646603e-05, 8.45148228e-04, 2.48419237e-03, ...,\n",
       "         1.13727714e-04, 8.14071682e-05, 1.02118844e-04],\n",
       "        [1.69759165e-04, 6.60383003e-03, 9.06272084e-02, ...,\n",
       "         6.68481061e-06, 8.84967085e-06, 6.52177323e-06],\n",
       "        [3.92694463e-04, 7.81763941e-02, 6.41846061e-02, ...,\n",
       "         1.39410677e-05, 1.04691553e-05, 6.84323504e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57941983e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75463099e-04, 1.69926541e-04, ...,\n",
       "         1.57768042e-07, 4.25027196e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607730e-07, 4.24554401e-08, 5.40797309e-08]],\n",
       "\n",
       "       [[8.68646239e-05, 8.45148286e-04, 2.48419354e-03, ...,\n",
       "         1.13727670e-04, 8.14071318e-05, 1.02118807e-04],\n",
       "        [1.69759063e-04, 6.60382910e-03, 9.06272829e-02, ...,\n",
       "         6.68481016e-06, 8.84966994e-06, 6.52177232e-06],\n",
       "        [3.92694405e-04, 7.81764165e-02, 6.41845614e-02, ...,\n",
       "         1.39410713e-05, 1.04691590e-05, 6.84323732e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57941983e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75463099e-04, 1.69926541e-04, ...,\n",
       "         1.57768042e-07, 4.25027196e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607730e-07, 4.24554401e-08, 5.40797309e-08]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[8.68646530e-05, 8.45148170e-04, 2.48419330e-03, ...,\n",
       "         1.13727707e-04, 8.14071609e-05, 1.02118836e-04],\n",
       "        [1.69759107e-04, 6.60383143e-03, 9.06272680e-02, ...,\n",
       "         6.68480880e-06, 8.84967722e-06, 6.52177778e-06],\n",
       "        [3.92694463e-04, 7.81763718e-02, 6.41845912e-02, ...,\n",
       "         1.39410604e-05, 1.04691599e-05, 6.84323822e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57942139e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75463099e-04, 1.69926541e-04, ...,\n",
       "         1.57768042e-07, 4.25027196e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607730e-07, 4.24554401e-08, 5.40797309e-08]],\n",
       "\n",
       "       [[8.68646530e-05, 8.45148170e-04, 2.48419214e-03, ...,\n",
       "         1.13727707e-04, 8.14070736e-05, 1.02118836e-04],\n",
       "        [1.69759136e-04, 6.60382910e-03, 9.06272382e-02, ...,\n",
       "         6.68481016e-06, 8.84966994e-06, 6.52176641e-06],\n",
       "        [3.92694637e-04, 7.81763941e-02, 6.41845912e-02, ...,\n",
       "         1.39410604e-05, 1.04691599e-05, 6.84323822e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196305e-04, ...,\n",
       "         1.57942139e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75462866e-04, 1.69926381e-04, ...,\n",
       "         1.57768042e-07, 4.25026379e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607730e-07, 4.24554401e-08, 5.40797309e-08]],\n",
       "\n",
       "       [[8.68646675e-05, 8.45148345e-04, 2.48419377e-03, ...,\n",
       "         1.13727729e-04, 8.14071755e-05, 1.02118764e-04],\n",
       "        [1.69759078e-04, 6.60383189e-03, 9.06272903e-02, ...,\n",
       "         6.68480743e-06, 8.84968358e-06, 6.52177641e-06],\n",
       "        [3.92694463e-04, 7.81763941e-02, 6.41846210e-02, ...,\n",
       "         1.39410677e-05, 1.04691553e-05, 6.84323504e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75835977e-04, 1.70196145e-04, ...,\n",
       "         1.57941983e-07, 4.25537969e-08, 5.42084280e-08],\n",
       "        [9.97940242e-01, 2.75462866e-04, 1.69926381e-04, ...,\n",
       "         1.57767914e-07, 4.25026379e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607715e-07, 4.24554436e-08, 5.40797309e-08]]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encdec_rnn_model.predict(tmp_x[500:550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e193f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[8.68646530e-05, 8.45148170e-04, 2.48419214e-03, ...,\n",
       "         1.13727707e-04, 8.14071609e-05, 1.02118742e-04],\n",
       "        [1.69759078e-04, 6.60382910e-03, 9.06272456e-02, ...,\n",
       "         6.68480743e-06, 8.84967540e-06, 6.52177641e-06],\n",
       "        [3.92694463e-04, 7.81763941e-02, 6.41845986e-02, ...,\n",
       "         1.39410604e-05, 1.04691599e-05, 6.84323822e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75835977e-04, 1.70195985e-04, ...,\n",
       "         1.57941983e-07, 4.25537969e-08, 5.42084280e-08],\n",
       "        [9.97940242e-01, 2.75462866e-04, 1.69926381e-04, ...,\n",
       "         1.57767914e-07, 4.25026379e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116530e-04, 1.69675521e-04, ...,\n",
       "         1.57607872e-07, 4.24554401e-08, 5.40798339e-08]],\n",
       "\n",
       "       [[8.68646675e-05, 8.45148345e-04, 2.48419377e-03, ...,\n",
       "         1.13727729e-04, 8.14070954e-05, 1.02118858e-04],\n",
       "        [1.69759078e-04, 6.60382910e-03, 9.06272903e-02, ...,\n",
       "         6.68480743e-06, 8.84967540e-06, 6.52177641e-06],\n",
       "        [3.92694754e-04, 7.81763941e-02, 6.41846210e-02, ...,\n",
       "         1.39410704e-05, 1.04691580e-05, 6.84323641e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57941983e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75463099e-04, 1.69926541e-04, ...,\n",
       "         1.57768042e-07, 4.25027196e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607872e-07, 4.24554401e-08, 5.40797309e-08]],\n",
       "\n",
       "       [[8.68646675e-05, 8.45148345e-04, 2.48419261e-03, ...,\n",
       "         1.13727729e-04, 8.14071755e-05, 1.02118858e-04],\n",
       "        [1.69759136e-04, 6.60383236e-03, 9.06272754e-02, ...,\n",
       "         6.68481653e-06, 8.84967812e-06, 6.52177278e-06],\n",
       "        [3.92694405e-04, 7.81763867e-02, 6.41845912e-02, ...,\n",
       "         1.39410586e-05, 1.04691590e-05, 6.84323732e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57942139e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75463099e-04, 1.69926541e-04, ...,\n",
       "         1.57768042e-07, 4.25027196e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607730e-07, 4.24554401e-08, 5.40798339e-08]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[8.68647039e-05, 8.45148286e-04, 2.48419354e-03, ...,\n",
       "         1.13727670e-04, 8.14071318e-05, 1.02118807e-04],\n",
       "        [1.69759092e-04, 6.60383096e-03, 9.06272531e-02, ...,\n",
       "         6.68481516e-06, 8.84967631e-06, 6.52177096e-06],\n",
       "        [3.92694637e-04, 7.81763718e-02, 6.41845986e-02, ...,\n",
       "         1.39410604e-05, 1.04691599e-05, 6.84323822e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57941983e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75463099e-04, 1.69926541e-04, ...,\n",
       "         1.57768042e-07, 4.25027196e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607872e-07, 4.24554401e-08, 5.40797309e-08]],\n",
       "\n",
       "       [[8.68646312e-05, 8.45148345e-04, 2.48419400e-03, ...,\n",
       "         1.13727685e-04, 8.14071391e-05, 1.02118815e-04],\n",
       "        [1.69759180e-04, 6.60383003e-03, 9.06272978e-02, ...,\n",
       "         6.68481243e-06, 8.84968085e-06, 6.52177414e-06],\n",
       "        [3.92694463e-04, 7.81763494e-02, 6.41846135e-02, ...,\n",
       "         1.39410604e-05, 1.04691599e-05, 6.84323822e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57941983e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75463099e-04, 1.69926541e-04, ...,\n",
       "         1.57768042e-07, 4.25027196e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116530e-04, 1.69675521e-04, ...,\n",
       "         1.57607872e-07, 4.24554401e-08, 5.40798339e-08]],\n",
       "\n",
       "       [[8.68646239e-05, 8.45147937e-04, 2.48419354e-03, ...,\n",
       "         1.13727670e-04, 8.14071318e-05, 1.02118807e-04],\n",
       "        [1.69759107e-04, 6.60383096e-03, 9.06272605e-02, ...,\n",
       "         6.68481835e-06, 8.84968085e-06, 6.52177414e-06],\n",
       "        [3.92694463e-04, 7.81763718e-02, 6.41845912e-02, ...,\n",
       "         1.39410604e-05, 1.04691599e-05, 6.84323822e-06],\n",
       "        ...,\n",
       "        [9.97937679e-01, 2.75836239e-04, 1.70196145e-04, ...,\n",
       "         1.57941983e-07, 4.25538786e-08, 5.42085310e-08],\n",
       "        [9.97940242e-01, 2.75462866e-04, 1.69926381e-04, ...,\n",
       "         1.57767914e-07, 4.25026379e-08, 5.41419176e-08],\n",
       "        [9.97942984e-01, 2.75116268e-04, 1.69675361e-04, ...,\n",
       "         1.57607872e-07, 4.24554436e-08, 5.40797309e-08]]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd4c997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "4\n",
      "18\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "How                              \n"
     ]
    }
   ],
   "source": [
    "RX=\"\"\n",
    "for prediction in np.argmax(pred[40], 1):\n",
    "        print(prediction)\n",
    "        dc=logits_to_text(prediction,x_tk)\n",
    "        dc=str(dc)\n",
    "#         print(dc)\n",
    "        if dc!=None:\n",
    "#             RX+=dc+\" \"\n",
    "            RX+=dc\n",
    "print(RX) \n",
    "sentence = \" \".join(RX.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c1364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8313b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b544a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9984410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
